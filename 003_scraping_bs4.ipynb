{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nswzAJ7UJqVM",
        "rRDx126YK6MN",
        "q1csyjjfGRw0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fleshgordo/scrapinghub/blob/main/003_scraping_bs4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping a website without an API with Beautifulsoup\n"
      ],
      "metadata": {
        "id": "LxknVYOZFjw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requesting a website\n",
        "\n",
        "In order to entirely download a webpage and its content we first need to request the server, wait for the response and store it in a python variable. This is achieved with the [requests](https://pypi.org/project/requests/) library. Before using it, we need to import it to our current runtime (this needs to be done only once!)"
      ],
      "metadata": {
        "id": "nswzAJ7UJqVM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BahpnCqCFe5I"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the [quickstart tutorial](https://requests.readthedocs.io/en/latest/user/quickstart/) we can fetch a website that interests us. In this case we will scrape the very first webpage that went online (in CERN Geneva 1989)"
      ],
      "metadata": {
        "id": "aKuNz-JAKBVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get('http://info.cern.ch/hypertext/WWW/TheProject.html')\n",
        "print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXyxzzVSKA39",
        "outputId": "1a6d1789-289c-48f5-9cbf-a03ce2dedd85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code should output `Response [200]`. To output the HTML source code of the page we need to access the `text` property. The response will be stored in a variable called `source`."
      ],
      "metadata": {
        "id": "jZ20yZKsKc83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(r.text)\n",
        "source = r.text"
      ],
      "metadata": {
        "id": "inYzlaMmKoxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BeautifulSoup \n",
        "\n",
        "The code is highly unreadable. Parsing through this source code is tedious and quickly time-consuming. Hence, [Beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) comes into play. This library is known for extracting data out of web pages. It provides elegant ways of navigating, searching, and modifying the parse tree of HTML and XML files. It commonly saves programmers hours or days of work. So, let's import this library:"
      ],
      "metadata": {
        "id": "rRDx126YK6MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "3dxWUjWIK5x1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our source code will be loaded into the Beautifulsoup which creates a python object that becomes browsable instead of a basic text string."
      ],
      "metadata": {
        "id": "K_8klqLHMFhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(source, 'html.parser')\n",
        "print(soup)"
      ],
      "metadata": {
        "id": "lYvnH3GqMaKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the function `prettify()` the output looks a bit cleaner:"
      ],
      "metadata": {
        "id": "38K38GbvCqC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "4DBEomEZCxZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the output of the new `soup` variable looks pretty much the same as the `source`, its major difference is that it is a python object that contains some functions in order to access the HTML structure. Let's say, we are interested only into the hyperlinks that are present on the page:"
      ],
      "metadata": {
        "id": "unUuKlZzMFY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(\"a\")"
      ],
      "metadata": {
        "id": "OwSro7NMNBs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also search only for specific HTML tags such as `<h1>` or `<p> ` \n"
      ],
      "metadata": {
        "id": "-0sjzNzNNtIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlines = soup.find_all(\"h1\")\n",
        "texts = soup.find_all(\"p\")"
      ],
      "metadata": {
        "id": "5S6ub1foN3nT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(headlines)"
      ],
      "metadata": {
        "id": "ACYqUcH_OZw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the time of writing, the webpage consists only of one headline `<h1>`. The text is technically an element of a list that has only one entry. In order to acces the first element of that list, we need to call it's array index:"
      ],
      "metadata": {
        "id": "gNljY5UlbZnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(headlines[0])"
      ],
      "metadata": {
        "id": "U9EHxORRbwJc",
        "outputId": "4625051e-4816-4634-a5c1-ea2193549ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h1>World Wide Web</h1>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is still wrapped around the html tags h1. If we want to access the \"pure\" text we can make use of the function getText():"
      ],
      "metadata": {
        "id": "nMEm5LhvcOH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(headlines[0].getText())"
      ],
      "metadata": {
        "id": "VzpS6asDcbCb",
        "outputId": "a9722fde-965b-4c36-d28c-adb136b71880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "World Wide Web\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary scraping CERN website\n",
        "\n",
        "To summarize all the actions it took to:\n",
        "\n",
        "1.   fetch the website content\n",
        "2.   transform source code into beautifulsoup element\n",
        "3.   find only h1 tags (titles)\n",
        "4.   print only the headline\n",
        "\n",
        "we could write:"
      ],
      "metadata": {
        "id": "EYVf_7Rqcg3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # import necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "r = requests.get('http://info.cern.ch/hypertext/WWW/TheProject.html') # fetch the website\n",
        "source = r.text # store its response in variables source\n",
        "\n",
        "soup = BeautifulSoup(source, 'html.parser') # parse the webpage source into a bs4 soup object \n",
        "headlines = soup.find_all(\"h1\") # search for h1 tags\n",
        "\n",
        "print(headlines[0].getText()) # get the text for the first found h1 tag"
      ],
      "metadata": {
        "id": "TTjkoHsGcza8",
        "outputId": "cea02bfc-a788-4fb2-9978-34101aac9843",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "World Wide Web\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look only at the links that are present on the website:"
      ],
      "metadata": {
        "id": "OPNENGrqdese"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = soup.find_all(\"a\") # find all links on webpage\n",
        "print(links[0].getText()) # show text of first link in list"
      ],
      "metadata": {
        "id": "RH9jXHqtmWQW",
        "outputId": "cab72c3e-989c-40a1-a817-842a0c120894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hypermedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more than one link in the webpage source. To see how many elements are in that list that we call `links`  we can print its amount of elements with the `len()` function:"
      ],
      "metadata": {
        "id": "tP6GG3v4dryd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(links))"
      ],
      "metadata": {
        "id": "XaWHBdWsd6-V",
        "outputId": "3b490ef0-da9c-474f-f923-6298ea22cfb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the time of writing, there are 25 links. Let's create a loop of that list. Study this [python tutorial](https://www.w3schools.com/python/python_lists_loop.asp) for looping a list:"
      ],
      "metadata": {
        "id": "Ic7z9IuDd9VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for link in links:\n",
        "  print(link)"
      ],
      "metadata": {
        "id": "ouGuGLWleFrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can again use the `getText()` function to extract only the names of the link. See the [documentation for beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to discover other useful functions for extracting data."
      ],
      "metadata": {
        "id": "9JukK4uKecHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for link in links:\n",
        "  print(link.getText())"
      ],
      "metadata": {
        "id": "ES27m8-Ae0Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fake-headers\n",
        "\n",
        "It might be important now to be careful about sending the right user-agent since some website won't serve the content if the server detects that the request is coming from a python script. In this case, we need to adapt the request function a little bit. \n",
        "\n",
        "First, we need to define \"fake\" header information with a user-agent that looks inconspicous (chrome webbrowser on a macintosh computer). I copy/pasted this header from a standard web-browser.  "
      ],
      "metadata": {
        "id": "UiYNByDke7sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n"
      ],
      "metadata": {
        "id": "w0Qkxhy_frd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This headers variable needs to be send together with the requests function that fetches the website. That's it! No one will think again that you are a bot! Be sure to always define that fake-header in your requests:"
      ],
      "metadata": {
        "id": "ePNHSG9af9k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(\"https://google.ch/\",headers=headers) # fetch the website"
      ],
      "metadata": {
        "id": "1G0-CXO7gEwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape a newspage\n",
        "\n",
        "In this example we will scrape a newspaper website and try to extract only the headlines.\n"
      ],
      "metadata": {
        "id": "8gpkQYHWfuoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # import necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "my_url = \"https://nzz.ch/\"\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "\n",
        "r = requests.get(my_url,headers=headers) # fetch the website\n",
        "source = r.text # store its response in variables source\n",
        "\n",
        "soup = BeautifulSoup(source, 'html.parser') # parse the webpage source into a bs4 soup object \n",
        "print(soup.prettify())\n"
      ],
      "metadata": {
        "id": "6OhIVkI-fHzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the page source in the web-inspector first to analyze the website. As a reminder on how to use web inspector watch this [tutorial](https://www.youtube.com/watch?v=TuZJD-lKjCo)"
      ],
      "metadata": {
        "id": "f7l06DHPJSN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# to extract only the teaser titles from the news, it became clear\n",
        "# that all headlines are in <h2> tags that contained the class=\"teaser__title\" attribute\n",
        "# with bs4 one can filter only those tags with the line below:\n",
        "\n",
        "teasers = soup.find_all(\"h2\", {\"class\": \"teaser__title\"}) \n",
        "#print(teasers)\n",
        "print(f\"there are {len(teasers)} headlines on the page\") # with len we get the length of the list\n",
        "for teaser in teasers:\n",
        "  print(teaser.getText())"
      ],
      "metadata": {
        "id": "VLZOBQ7tIVkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you try to find a simple website and extract some information from it? Make sure to not choose a too complex page or a platform where you need to login. \n",
        "\n",
        "Start with this code snippet:"
      ],
      "metadata": {
        "id": "d2gnfWSgeMbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # import necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "my_url = \"YOUR_URL_HERE\"\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "\n",
        "r = requests.get(my_url,headers=headers) # fetch the website\n",
        "source = r.text # store its response in variables source\n",
        "\n",
        "soup = BeautifulSoup(source, 'html.parser') # parse the webpage source into a bs4 soup object \n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "Ig8WGEqxeQeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continue to analyze your soup here:\n",
        "\n"
      ],
      "metadata": {
        "id": "VpNKCYV4eTrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving output to a file\n",
        "\n",
        "To write all headlines into CSV file, we can use the python csv module. This is for demonstration only and the CSV will only contain one column, but the principles are the same if you have more data to write to this file"
      ],
      "metadata": {
        "id": "r9GEyt3yc3YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "outputfile = \"sample_data/output.csv\" # filepath\n",
        "\n",
        "# Open a CSV file for writing\n",
        "with open(outputfile, 'w', newline='') as file:\n",
        "    # Create a writer object\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write each string to a new row in the CSV file\n",
        "    for teaser in teasers:\n",
        "        writer.writerow([teaser.getText()])"
      ],
      "metadata": {
        "id": "zZ6VyEg3dGTn"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}